{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T18:55:51.876949Z",
     "iopub.status.busy": "2024-01-18T18:55:51.876467Z",
     "iopub.status.idle": "2024-01-18T18:55:55.520472Z",
     "shell.execute_reply": "2024-01-18T18:55:55.519167Z",
     "shell.execute_reply.started": "2024-01-18T18:55:51.876911Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "# 1. to handle the data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# to visualize the dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# To preprocess the data\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "# import iterative imputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# machine learning\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "#for classification tasks\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, RandomForestRegressor\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "#metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# ignore warnings   \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T18:59:32.240208Z",
     "iopub.status.busy": "2024-01-18T18:59:32.2397Z",
     "iopub.status.idle": "2024-01-18T18:59:32.294972Z",
     "shell.execute_reply": "2024-01-18T18:59:32.293663Z",
     "shell.execute_reply.started": "2024-01-18T18:59:32.240168Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# load the data from csv file placed locally in our pc\n",
    "df = pd.read_csv('./heart_disease_uci.csv')\n",
    "\n",
    "# print the first 5 rows of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Age Columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploring the datatype of each column\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data shpae\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id column\n",
    "df['id'].min(), df['id'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# age column\n",
    "df['age'].min(), df['age'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's summarie the age column\n",
    "df['age'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw a histogram to see the distribution of age column\n",
    "sns.histplot(df['age'], kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the mean, median and mode of age column using sns\n",
    "sns.histplot(df['age'], kde=True)\n",
    "plt.axvline(df['age'].mean(), color='red')\n",
    "plt.axvline(df['age'].median(), color='green')\n",
    "plt.axvline(df['age'].mode()[0], color='blue')\n",
    "\n",
    "# print the value of mean, median and mode of age column\n",
    "print('Mean:', df['age'].mean())\n",
    "print('Median:', df['age'].median())\n",
    "print('Mode:', df['age'].mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Let's explore the gender based distribution of the dataset for age column.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the histogram of age column using plotly and coloring this by sex\n",
    "\n",
    "fig = px.histogram(data_frame=df, x='age', color='sex')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the values of sex column\n",
    "df['sex'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the percentages of male and female value counts in the data\n",
    "male_count = 726\n",
    "female_count = 194\n",
    "total_count = male_count + female_count\n",
    "\n",
    "# calculate percentages\n",
    "male_percentage = (male_count / total_count) * 100\n",
    "female_percentage = (female_count / total_count) * 100\n",
    "\n",
    "# display the results\n",
    "print(f\"Male percentage in the data: {male_percentage:.2f}%\")\n",
    "print(f\"Female Percentage in the data: {female_percentage:.2f}%\")\n",
    "\n",
    "# difference\n",
    "difference_percentage = ((male_count - female_count) / female_count) * 100\n",
    "print(f\"Males are {difference_percentage:.2f}% more than females in the data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Same Plot Using Plotly and Coloring this by sex\n",
    "fig = px.pie(df, names='sex', color='sex')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the values count of age column grouping by sex column\n",
    "df.groupby('sex')['age'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Dataset Column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets deal with dataset column\n",
    "# find the unique values in dataset column\n",
    "df['dataset'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find unique values count in dataset column\n",
    "df['dataset'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the countplot of dataset column\n",
    "# sns.countplot(data=df, x='dataset', hue = 'sex')\n",
    "\n",
    "# better plots with plotly\n",
    "fig = px.bar(df, x='dataset', color='sex')\n",
    "fig.show()\n",
    "\n",
    "# print the values count of dataset column grouped by sex\n",
    "print(df.groupby('sex')['dataset'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a plot of age column using plotly and coloring this by dataset column\n",
    "fig = px.histogram(data_frame=df, x='age', color='dataset')\n",
    "fig.show()\n",
    "\n",
    "# print the mean median and mode of age column grouped by dataset column\n",
    "print(f\"Mean of Data Set: {df.groupby('dataset')['age'].mean()}\")\n",
    "print(\"-------------------------------------\")\n",
    "print(f\"Median of Data Set: {df.groupby('dataset')['age'].median()}\")\n",
    "print(\"-------------------------------------\")\n",
    "print(f\"Mode of Data Set: {df.groupby('dataset')['age'].agg(pd.Series.mode)}\")\n",
    "print(\"-------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Let's explore cp (Chest Pain) column:** \n",
    "\n",
    " **Types of Chest pain :**\n",
    "\n",
    "    1. Asymptomatic: No chest pain or discomfort.\n",
    "    2. Non-Anginal: Chest pain not typical of heart-related issues; requires further investigation.\n",
    "    3. Atypical Angina: Chest pain with characteristics different from typical heart-related chest pain.\n",
    "    4. Typical Angina: Classic chest pain indicating potential insufficient blood supply to the heart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# value count of cp column\n",
    "df['cp'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_num = pd.crosstab(df.cp,df.num)\n",
    "cp_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Some Helpful info\n",
    "pd.crosstab(df.cp,df.num).plot(kind=\"bar\",figsize=(10,6), \n",
    "                               color = ['salmon','blue','pink','lightblue','green'])\n",
    "plt.title('Heart disease frequency for Chest Pain type')\n",
    "plt.xlabel('Chest Pain type')\n",
    "plt.ylabel('Amount')\n",
    "plt.legend(['No Disease', 'Stage 1', 'Stage 2', 'Stage 3', 'Stage 4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw the plot of age column grouped by cp column using plotly\n",
    "fig = px.histogram(data_frame=df, x='cp', color='sex')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count plot of cp column by dataset column\n",
    "sns.countplot(df, x='cp', hue='dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw the plot of age column grouped by cp column using plotly\n",
    "fig = px.histogram(data_frame=df, x='age', color='cp')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw the plot of cp column grouped by num column using plotly\n",
    "fig = px.histogram(data_frame=df, x='cp', color='num')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Let'e explore the trestbps (resting blood pressure) column:**\n",
    "\n",
    "The normal resting blood pressure is 120/80 mm Hg.\n",
    "\n",
    "high blood pressure increasing the risk of heart disease and stroke, often asymptomatic, while low blood pressure can lead to dizziness and fainting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the value counts of trestbps column\n",
    "df['trestbps'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Resting Blood Pressure vs Disease\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Box(y=df['trestbps'].values , name='BP at Rest for all', marker_color = 'green',boxmean=True))\n",
    "fig.add_trace(go.Box(y=df[df['num']== 0]['trestbps'].values, name ='No Disease', marker_color = 'blue', boxmean = True))\n",
    "fig.add_trace(go.Box(y=df[df['num'] !=0]['trestbps'].values, name ='Heart Disease', marker_color = 'red', boxmean = True))\n",
    "fig.update_layout(title = 'BP Distribution (at rest)', yaxis_title = 'Blood Pressure (mm/Hg)', title_x = 0.5)\n",
    "fig.update_xaxes(showline=True, linewidth=2, linecolor='black', mirror=True)\n",
    "fig.update_yaxes(showline=True, linewidth=2, linecolor='black', mirror=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a histplot of trestbps column\n",
    "sns.histplot(df['trestbps'], kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Lets Explore the chol Column**\n",
    "\n",
    "What is the chol :  a fatty substance essential for body function, but elevated levels can contribute to heart disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we Describe Chol\n",
    "df['chol'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Tab of chol and num\n",
    "cross = pd.crosstab(df['chol'], df['num']).describe()\n",
    "cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(y=df['chol'], hue=df['num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw the plot of chol column grouped by num column using plotly\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Violin(y=df['chol'].values , name='All Patient', marker_color = 'green'))\n",
    "fig.add_trace(go.Violin(y=df[df['num']== 0]['chol'].values, name ='No Disease', marker_color = 'blue'))\n",
    "fig.add_trace(go.Violin(y=df[df['num'] == 4]['chol'].values, name ='Heart Disease', marker_color = 'red'))\n",
    "fig.update_layout(title = 'Cholesterol Level Distribution', yaxis_title = 'Cholesterol Level', title_x = 0.5 )\n",
    "fig.update_xaxes(showline=True, linewidth=2, linecolor='black', mirror=True)\n",
    "fig.update_yaxes(showline=True, linewidth=2, linecolor='black', mirror=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Lets Explore Thal ( Thalesmia)**\n",
    "\n",
    "    Normal: Within expected or healthy parameters.\n",
    "\n",
    "    Reversible Defect: An abnormality that can potentially be corrected or improved.\n",
    "\n",
    "    Fixed Defect: An abnormality that is unlikely to change or be corrected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['thal'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by thal by sex\n",
    "df.groupby('thal')['sex'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby Thal by Dataset\n",
    "df.groupby('thal')['dataset'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count plot of cp column by dataset column\n",
    "sns.countplot(df, x='thal', hue='sex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw the plot of thal column grouped by age column using plotly\n",
    "fig = px.histogram(data_frame=df, x='age', color='thal')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw the plot of thal column grouped by age column using plotly\n",
    "fig = px.histogram(data_frame=df, x='thal', color='dataset')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot or groupby to check the people who have thal does the have cp \n",
    "df.groupby('thal')['cp'].value_counts()\n",
    "# Plot to Visualize\n",
    "sns.countplot(df, x='thal', hue='cp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now Check People with Thal Survive or Not \n",
    "df.groupby('thal')['num'].value_counts()\n",
    "# Plot to Visualize\n",
    "sns.countplot(df, x='thal', hue='num' , palette='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Lets Deal With Num , The Target Variable**\n",
    "   * `0 = no heart disease`\n",
    "   * `1 = mild heart disease`\n",
    "   * `2 = moderate heart disease `\n",
    "   * `3 = severe heart disease`\n",
    "   * `4 = critical heart disease `\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby num with sex \n",
    "df.groupby('num')['sex'].value_counts()\n",
    "# Plot to Visualize\n",
    "sns.countplot(df, x='num', hue='sex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupby num by age \n",
    "df.groupby('num')['age'].value_counts()\n",
    "# Plot to Visualize\n",
    "sns.histplot(df, x='age', hue='num')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Histplot using Plotly \n",
    "px.histogram(data_frame=df, x='age', color='num')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()[df.isnull().sum() > 0].sort_values(ascending=False)\n",
    "missing_data_cols = df.isnull().sum()[df.isnull().sum() > 0].index.tolist()\n",
    "missing_data_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['thal'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['thal', 'ca', 'slope', 'exang', 'restecg','fbs', 'cp', 'sex', 'num']\n",
    "bool_cols = ['fbs', 'exang']\n",
    "numeric_cols = ['oldpeak', 'thalch', 'chol', 'trestbps', 'age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function to impute the missing values in thal column\n",
    "\n",
    "def impute_categorical_missing_data(passed_col):\n",
    "    \n",
    "    df_null = df[df[passed_col].isnull()]\n",
    "    df_not_null = df[df[passed_col].notnull()]\n",
    "\n",
    "    X = df_not_null.drop(passed_col, axis=1)\n",
    "    y = df_not_null[passed_col]\n",
    "    \n",
    "    other_missing_cols = [col for col in missing_data_cols if col != passed_col]\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype == 'object' or X[col].dtype == 'category':\n",
    "            X[col] = label_encoder.fit_transform(X[col])\n",
    "\n",
    "    if passed_col in bool_cols:\n",
    "        y = label_encoder.fit_transform(y)\n",
    "        \n",
    "    iterative_imputer = IterativeImputer(estimator=RandomForestRegressor(random_state=42), add_indicator=True)\n",
    "\n",
    "    for col in other_missing_cols:\n",
    "        if X[col].isnull().sum() > 0:\n",
    "            col_with_missing_values = X[col].values.reshape(-1, 1)\n",
    "            imputed_values = iterative_imputer.fit_transform(col_with_missing_values)\n",
    "            X[col] = imputed_values[:, 0]\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    rf_classifier = RandomForestClassifier()\n",
    "\n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "    acc_score = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(\"The feature '\"+ passed_col+ \"' has been imputed with\", round((acc_score * 100), 2), \"accuracy\\n\")\n",
    "\n",
    "    X = df_null.drop(passed_col, axis=1)\n",
    "\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype == 'object' or X[col].dtype == 'category':\n",
    "            X[col] = label_encoder.fit_transform(X[col])\n",
    "\n",
    "    for col in other_missing_cols:\n",
    "        if X[col].isnull().sum() > 0:\n",
    "            col_with_missing_values = X[col].values.reshape(-1, 1)\n",
    "            imputed_values = iterative_imputer.fit_transform(col_with_missing_values)\n",
    "            X[col] = imputed_values[:, 0]\n",
    "        else:\n",
    "            pass\n",
    "                \n",
    "    if len(df_null) > 0: \n",
    "        df_null[passed_col] = rf_classifier.predict(X)\n",
    "        if passed_col in bool_cols:\n",
    "            df_null[passed_col] = df_null[passed_col].map({0: False, 1: True})\n",
    "        else:\n",
    "            pass\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    df_combined = pd.concat([df_not_null, df_null])\n",
    "    \n",
    "    return df_combined[passed_col]\n",
    "\n",
    "def impute_continuous_missing_data(passed_col):\n",
    "    \n",
    "    df_null = df[df[passed_col].isnull()]\n",
    "    df_not_null = df[df[passed_col].notnull()]\n",
    "\n",
    "    X = df_not_null.drop(passed_col, axis=1)\n",
    "    y = df_not_null[passed_col]\n",
    "    \n",
    "    other_missing_cols = [col for col in missing_data_cols if col != passed_col]\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype == 'object' or X[col].dtype == 'category':\n",
    "            X[col] = label_encoder.fit_transform(X[col])\n",
    "    \n",
    "    iterative_imputer = IterativeImputer(estimator=RandomForestRegressor(random_state=42), add_indicator=True)\n",
    "\n",
    "    for col in other_missing_cols:\n",
    "        if X[col].isnull().sum() > 0:\n",
    "            col_with_missing_values = X[col].values.reshape(-1, 1)\n",
    "            imputed_values = iterative_imputer.fit_transform(col_with_missing_values)\n",
    "            X[col] = imputed_values[:, 0]\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    rf_regressor = RandomForestRegressor()\n",
    "\n",
    "    rf_regressor.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = rf_regressor.predict(X_test)\n",
    "\n",
    "    print(\"MAE =\", mean_absolute_error(y_test, y_pred), \"\\n\")\n",
    "    print(\"RMSE =\", mean_squared_error(y_test, y_pred, squared=False), \"\\n\")\n",
    "    print(\"R2 =\", r2_score(y_test, y_pred), \"\\n\")\n",
    "\n",
    "    X = df_null.drop(passed_col, axis=1)\n",
    "\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype == 'object' or X[col].dtype == 'category':\n",
    "            X[col] = label_encoder.fit_transform(X[col])\n",
    "\n",
    "    for col in other_missing_cols:\n",
    "        if X[col].isnull().sum() > 0:\n",
    "            col_with_missing_values = X[col].values.reshape(-1, 1)\n",
    "            imputed_values = iterative_imputer.fit_transform(col_with_missing_values)\n",
    "            X[col] = imputed_values[:, 0]\n",
    "        else:\n",
    "            pass\n",
    "                \n",
    "    if len(df_null) > 0: \n",
    "        df_null[passed_col] = rf_regressor.predict(X)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    df_combined = pd.concat([df_not_null, df_null])\n",
    "    \n",
    "    return df_combined[passed_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()[df.isnull().sum() > 0].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# impute missing values using our functions\n",
    "for col in missing_data_cols:\n",
    "    print(\"Missing Values\", col, \":\", str(round((df[col].isnull().sum() / len(df)) * 100, 2))+\"%\")\n",
    "    if col in categorical_cols:\n",
    "        df[col] = impute_categorical_missing_data(col)\n",
    "    elif col in numeric_cols:\n",
    "        df[col] = impute_continuous_missing_data(col)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again CHecking Missing Values \n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here we are Done With Imputing Missing Values , By using Advance Methods Like Random Forest and Iterative Imputer . Which Are More Accurate then using Mean , Median or Mode  We Define a FUnction for Iputing Missing Values , In Which We Passed the Columns Names and The FUnction Return a Dataset With no Missing Values .\n",
    "- \n",
    "      MEthods : \n",
    "      1. Random Forest Classifier \n",
    "      2. Random Forest Regressor\n",
    "      3. Iterative Imputer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create box plots for all numeric columns using for loop and subplot\n",
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "colors = ['red', 'green', 'blue', 'orange', 'purple']\n",
    "\n",
    "for i, col in enumerate(numeric_cols):\n",
    "    plt.subplot(3, 2, i+1)\n",
    "    sns.boxplot(x=df[col], color=colors[i])\n",
    "    plt.title(col)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the row from df where trestbps value is 0\n",
    "df[df['trestbps'] == 0]\n",
    "# remove this row from data\n",
    "df = df[df['trestbps'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving The Cleaned Data \n",
    "df.to_csv('heart_disease_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- While Dealing with Outliers , from my Observations There is only One Outlier in the dataset which i removed . Other Values Have some Meaningfull Insight , so we Cannot remove them . Leave them in the Dataset .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " The Target Column is `num` which is the predicted attribute. We will use this column to predict the heart disease. \n",
    " The unique values in this column are: [0, 1].\n",
    "\n",
    "0 = no heart disease\n",
    "1 = heart disease\n",
    "\n",
    "The models that you will use to predict the heart disease. These models should be classifiers for multi-class classification.\n",
    "\n",
    "1. Random Forest\n",
    "2. XGB Classifier.</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T18:59:55.997777Z",
     "iopub.status.busy": "2024-01-18T18:59:55.997362Z",
     "iopub.status.idle": "2024-01-18T18:59:56.019983Z",
     "shell.execute_reply": "2024-01-18T18:59:56.019037Z",
     "shell.execute_reply.started": "2024-01-18T18:59:55.997748Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import Libraires \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Train test Split \n",
    "from sklearn.model_selection import train_test_split\n",
    "# Models \n",
    "from sklearn.naive_bayes import GaussianNB , BernoulliNB , MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier , DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier , AdaBoostClassifier ,RandomForestRegressor , AdaBoostRegressor\n",
    "from xgboost import XGBClassifier , XGBRegressor\n",
    "from sklearn.linear_model import LinearRegression ,LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor , KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor , GradientBoostingClassifier\n",
    "from sklearn.svm import SVC , SVR\n",
    "from xgboost import XGBClassifier , XGBRegressor\n",
    "# Import Naive Bayes\n",
    "#metrics\n",
    "from sklearn.metrics import mean_squared_error , mean_absolute_error , r2_score , classification_report , accuracy_score , f1_score , precision_score\n",
    "#import grid search cv for cross validation\n",
    "from sklearn.model_selection import GridSearchCV , RandomizedSearchCV\n",
    "# import preprocessors\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder , OneHotEncoder\n",
    "from sklearn.preprocessing import QuantileTransformer , PowerTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "# Remove Warning \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Saving Model \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T19:00:06.419641Z",
     "iopub.status.busy": "2024-01-18T19:00:06.417343Z",
     "iopub.status.idle": "2024-01-18T19:00:06.452547Z",
     "shell.execute_reply": "2024-01-18T19:00:06.451287Z",
     "shell.execute_reply.started": "2024-01-18T19:00:06.419579Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load Again The Clean Data \n",
    "data = df.copy()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T19:00:37.47653Z",
     "iopub.status.busy": "2024-01-18T19:00:37.475467Z",
     "iopub.status.idle": "2024-01-18T19:00:37.511498Z",
     "shell.execute_reply": "2024-01-18T19:00:37.510407Z",
     "shell.execute_reply.started": "2024-01-18T19:00:37.47649Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# In some of the features, there is space will will create problem later on. \n",
    "# So we rename those attributes to handle problems in the future.\n",
    "data['thal'].replace({'fixed defect':'fixed_defect' , 'reversable defect': 'reversable_defect' }, inplace =True)\n",
    "data['cp'].replace({'typical angina':'typical_angina', 'atypical angina': 'atypical_angina' }, inplace =True)\n",
    "data['restecg'].replace({'normal': 'normal' , 'st-t abnormality': 'ST-T_wave_abnormality' , 'lv hypertrophy': 'left_ventricular_hypertrophy' }, inplace =True)\n",
    "\n",
    "# Genrating New Dataset with Less Columns Which Are Necessary .\n",
    "data_1 = data[['age','sex','cp','dataset', 'trestbps', 'chol', 'fbs','restecg' , 'thalch', 'exang', 'oldpeak', 'slope', 'ca', 'thal']].copy()\n",
    "# Some Changes in Target Variable | Only Two Categories (0,1) . 0 for No-Disease , 1 for Disease\n",
    "data_1['target'] = ((data['num'] > 0)*1).copy()\n",
    "# Encoding Sex \n",
    "data_1['sex'] = (data['sex'] == 'Male')*1\n",
    "# Encoding Fbs and exang\n",
    "data_1['fbs'] = (data['fbs'])*1\n",
    "data_1['exang'] = (data['exang'])*1\n",
    "# Renaming COlumns Names.\n",
    "data_1.columns = ['age', 'sex', 'chest_pain_type','country' ,'resting_blood_pressure', \n",
    "              'cholesterol', 'fasting_blood_sugar','Restecg',\n",
    "              'max_heart_rate_achieved', 'exercise_induced_angina', \n",
    "              'st_depression', 'st_slope_type', 'num_major_vessels', \n",
    "              'thalassemia_type', 'target']\n",
    "# Load Data Sample \n",
    "data_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest is an ensemble learning technique used for both classification and regression tasks. It builds multiple decision trees during training and merges their predictions to improve accuracy and reduce overfitting.\n",
    "\n",
    "1. High Accuracy\n",
    "2. Robust to Overfitting\n",
    "3. Handles Missing Values\n",
    "\n",
    "Random Forest is a versatile and powerful algorithm, especially effective in scenarios with high-dimensional data and complex relationships. It excels in situations where high accuracy is crucial, and its ability to handle missing values and resist overfitting makes it a popular choice in machine learning applications.</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_random_forest(data, target):\n",
    "    # Dictionary to store LabelEncoders for each categorical column\n",
    "    label_encoders = {}\n",
    "\n",
    "    # split the data into X and y\n",
    "    X = data.drop(target, axis=1)\n",
    "    y = data[target]\n",
    "\n",
    "    # Create a new LabelEncoder for each categorical column\n",
    "    for col in X.select_dtypes(include=['object', 'category']).columns:\n",
    "        le = LabelEncoder()\n",
    "        X[col] = le.fit_transform(X[col])\n",
    "        label_encoders[col] = le\n",
    "\n",
    "    # split the data into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "    # Scaling Data\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Define the Random Forest model\n",
    "    rf_model = RandomForestClassifier(random_state=0,class_weight='balanced')\n",
    "\n",
    "    # Define hyperparameters for tuning\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 150],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "\n",
    "    # Perform GridSearchCV for hyperparameter tuning\n",
    "    grid_search = GridSearchCV(rf_model, param_grid, cv=5, scoring='accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best model and parameters\n",
    "    best_rf_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    # Print the best hyperparameters\n",
    "    print('Best Hyperparameters:')\n",
    "    print(best_params)\n",
    "\n",
    "    # Train the model on the full training set\n",
    "    best_rf_model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    y_pred = best_rf_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(f'Accuracy on Test Set: {accuracy:.2f}')\n",
    "\n",
    "    # Inverse transform at the End\n",
    "    # Loop through each column to decode the data\n",
    "    for col, le in label_encoders.items():\n",
    "        # Use the inverse_transform method to decode the column in both training and test sets\n",
    "        X[col] = le.inverse_transform(X[col])\n",
    "\n",
    "    return best_rf_model, best_params, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_random_forest(data_1, 'target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgb_classifier(data, target):\n",
    "    # split the data into X and y\n",
    "    X = data.drop(target, axis=1)\n",
    "    y = data[target]\n",
    "    \n",
    "    # encode X data using separate label encoder for all categorical columns and save it for inverse transform\n",
    "    # Task: Separate Encoder for all cat and object columns and inverse transform at the end\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype == 'object' or X[col].dtype == 'category':\n",
    "            X[col] = label_encoder.fit_transform(X[col])\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    # split the data into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "    \n",
    "    # Scaling Data \n",
    "    scaler = MinMaxScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Define the XGBClassifier model\n",
    "    xgb_model = XGBClassifier(random_state=0)\n",
    "\n",
    "    # Define hyperparameters for tuning\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 150],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.8, 1.0],\n",
    "        'gamma': [0, 1, 2]\n",
    "    }\n",
    "\n",
    "    # Perform GridSearchCV for hyperparameter tuning\n",
    "    grid_search = GridSearchCV(xgb_model, param_grid, cv=5, scoring='accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best model and parameters\n",
    "    best_xgb_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    # Print the best hyperparameters\n",
    "    print('Best Hyperparameters:')\n",
    "    print(best_params)\n",
    "\n",
    "    # Train the model on the full training set\n",
    "    best_xgb_model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    y_pred = best_xgb_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'Accuracy on Test Set: {accuracy:.2f}')\n",
    "\n",
    "\n",
    "    return best_xgb_model, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_xgb_classifier(data_1,'target')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 888463,
     "sourceId": 1508992,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30635,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
